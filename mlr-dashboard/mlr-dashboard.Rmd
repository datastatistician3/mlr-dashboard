---
title: "Machine Learning Models"
params:
output:
  flexdashboard::flex_dashboard:
    social: [ "menu" ]
    orientation: rows
    vertical_layout: fill
    # source_code: embed
    theme: cerulean
runtime: shiny
---

<!--  Set the working directory to the repository's base directory; this assumes the report is nested inside of three directories.-->
```{r, echo=F, message=F}
# cat("Working directory: ", getwd())
library(knitr)
library(flexdashboard)
library(shiny)
library(caret)
library(dplyr)
opts_knit$set(root.dir='./../')  #Don't combine this call with any other chunk -especially one that uses file paths.
```

<!-- Set the report-wide options, and point to the external code file. -->
```{r set-options, echo=F}
# cat("Working directory: ", getwd())
report_render_start_time <- Sys.time()
opts_chunk$set(
  results      = 'show',
  comment      = NA,
  tidy         = FALSE,
  # dpi        = 400,
  # out.width  = "650px", #This affects only the markdown, not the underlying png file.  The height will be scaled appropriately.
  fig.width    = 4,
  fig.height   = 4,
  fig.path     = 'figure-dashboard-png/'
)

echo_chunks    <- FALSE #Toggle for debugging.
message_chunks <- FALSE #Toggle for debugging.
options(width=100) #So the output is 25% wider than the default.
```



```{r}
datasetss <- list(
  'none'= NULL,
  'iris'= datasets::iris,
  'cars'= datasets::mtcars,
  # 'diamonds'= ggplot2::diamonds,
  'Boston'=MASS::Boston
  # 'leaf'=leaf
  # 'midwest'=data.frame(midwest),
  # 'mpg'=data.frame(mpg),
  # 'msleep'=data.frame(msleep),
  # 'txhousing'=data.frame(txhousing)
)

mdls <- list('svmLinear'='svmLinear',
             'svmPoly'='svmPoly',
             'Neural Network'='nnet',
             'randomForest'='rf',
             'k-NN'='knn',
             'Naive Bayes'='nb'
             # 'GLM'='glm'
             )

tuneParams <- list(
  'svmLinear'=data.frame(C=c(0.01,0.1,1)),
  'svmPoly'= expand.grid(degree=1:3,scale=c(0.01,0.1),C=c(0.25,0.5,1)),
  'nnet'=expand.grid(size=c(1,3,5),decay=c(0.01,0.1,1)),
  'rf'=data.frame(mtry=c(2,3,4)),
  'knn'=data.frame(k=c(1,3,5,7,9)),
  'nb'=expand.grid(usekernel=c(T,F),adjust=c(0.01,0.1,1),fL=c(0.01,0.1,1))
  # 'glm'=NULL#data.frame()
)

mdli <- list(
  'Regression'=c(T,T,T,T,T,F),
  'Classification'=c(T,T,T,T,T,T)
)  

reg.mdls <- mdls[mdli[['Regression']]]
cls.mdls <- mdls[mdli[['Classification']]]

capitalize_each_word <- function(string, pattern = NULL){
  if (!is.null(pattern)) {
    g <- l <- j <- list()
    g <- stringr::str_split(base::trimws(string), pattern = pattern)
    for ( i in 1:length(g)){
      l[[i]] <- paste(Hmisc::capitalize(g[[i]]), collapse = " ")
      j[[i]] <- paste(l[[i]], collapse = " ")
    }
    } else {
    g <- l <- j <- list()
    g <- stringr::str_split(base::trimws(string), pattern = "\\s+")
    for ( i in 1:length(g)){
      l[[i]] <- paste(Hmisc::capitalize(g[[i]]), collapse = " ")
      j[[i]] <- paste(l[[i]], collapse = " ")
  }
}
  return(unlist(j))
}
```


<!-- Load 'sourced' R files.  Suppress the output when loading sources. -->
```{r load-sources, echo=echo_chunks, message=message_chunks}
```

<!-- Load packages, or at least verify they're available on the local machine.  Suppress the output when loading packages. -->
```{r load-packages, echo=echo_chunks, message=message_chunks}
```

<!-- Load any global functions and variables declared in the R file.  Suppress the output. -->
```{r declare-globals, echo=echo_chunks, results='show', message=message_chunks}
```

<!-- Declare any global functions specific to a Rmd output.  Suppress the output. -->
```{r rmd-specific, echo=echo_chunks, message=message_chunks}
```

<!-- Load the datasets.   -->
```{r load-data, echo=echo_chunks, results='show', message=message_chunks}
```

<!-- Tweak the datasets.   -->
```{r tweak-data, echo=echo_chunks, results='show', message=message_chunks}
```


Sidebar {.sidebar data-width=300}
=======================================================================

```{r}
library(shiny)
library(shinydashboard)
library(shinyjs)
require(shinyBS)
library("htmltools")
library("bsplus")
library(magrittr)
```

#### Data Input
```{r}
selectInput('dataset',label = 'Choose Dataset',choices = names(datasetss),selected='none')
fileInput('fileIn',label = 'Upload Data')
actionButton('btn_viewData',label = 'View Data',icon=icon('table'), width = '100%')
shiny::tags$br()
shiny::tags$br()
# selectizeInput('yvar',label='y (Choose Outcome Variable)',choices = character(0))
# selectizeInput('xvar',label='X (Select Predictor Variables)',choices = character(0),multiple = T)

```

```{r}
rawdata <- reactive({
  if (input$dataset == 'none'){
      input_file <- input$fileIn
      if(is.null(input_file)){return()}
      read.csv(
        file = input_file$datapath,
        sep = ','
      )
  } else {
    datasetss[[input$dataset]]
  }
})

bsModal('data',title = 'Dataset',trigger = 'btn_viewData',size = 'large',renderDataTable({rawdata()}))

renderUI({
  df <- rawdata()
  if (is.null(df)) return(NULL)

  items=names(df)
  names(items)=items
  selectInput(
    inputId = "yvar",
    label = "y (Choose Outcome Variable)",
    choices = items)
})

renderUI({
    df <- rawdata()
    if (is.null(df)) return(NULL)

    items=names(df)
    names(items)=items
    selectInput(
      inputId = "xvar",
      label = "X (Select Predictor Variables)",
      multiple = T,
      choices  = names(rawdata())[!names(rawdata()) %in% input$yvar],
      selected = names(rawdata())[!names(rawdata()) %in% input$yvar][1])
  })

reactive({
  observeEvent(input$dataset, {
   updateSelectizeInput(session,'yvar',choices=names(rawdata()),selected = names(rawdata())[1])
})
})

reactive({
  observeEvent(input$fileIn, {
   updateSelectizeInput(session,'yvar',choices=names(rawdata()),selected = names(rawdata())[1])
})
})

reactive({
observeEvent(input$yvar, {
  nms <- names(rawdata())[names(rawdata())!=input$yvar]
  updateSelectizeInput(session,'xvar',choices=nms,selected = nms)
})
})


dataTrain <- NULL
dataTest <- NULL

makeReactiveBinding('dataTrain')
makeReactiveBinding('dataTest')

observeEvent(input$rdo_model_type,{
  
  if(input$rdo_model_type=='Regression'){
    updateSelectizeInput(session,'slt_algo',choices = reg.mdls,selected = reg.mdls)
  } else {
    updateSelectizeInput(session,'slt_algo',choices = cls.mdls,selected = cls.mdls)
    
  }
})

observe({
    
    yvar <- input$yvar
    xvars <- input$xvar
    testsize <- input$sld_testsplit
    
    if(is.null(yvar)||yvar=='')
      return(NULL)
    
    # extract y and X from raw data
    y <- isolate(rawdata()[,yvar])
    X <-  isolate(rawdata()[,xvars])
    
    # deal with NA values
    yi <- !is.na(y)
    Xi <- complete.cases(X)
    
    df2 <- as.data.frame(cbind(y,X)[yi&Xi,])

    trainIndex <- caret::createDataPartition(df2$y,
                                      p = 1-(testsize/100),
                                      list = FALSE,
                                      times = 1)
    isolate({
      dataTrain <<- df2[ trainIndex,]
      dataTest  <<- df2[-trainIndex,]
    })
  })


train_models <- eventReactive(input$btn_train,{
    
    disable('btn_train')
    on.exit(enable('btn_train'))
    
    mdls <- isolate(input$slt_algo)

    fitControl <- caret::trainControl(method = "cv",savePredictions = T,
                           number = as.integer(input$rdo_CVtype),
                           summaryFunction = defaultSummary)
      
    trainArgs <- list(
    'svmLinear'= list(form=y ~ .,
                     data = dataTrain,
                     preProcess = c('scale','center'),
                     method = 'svmLinear',
                     trControl = fitControl,
                     tuneGrid=tuneParams[['svmLinear']]),
    'svmPoly'= list(form=y ~ .,
                    data = dataTrain,
                    preProcess = c('scale','center'),
                    method = 'svmPoly',
                    trControl = fitControl,
                    tuneGrid=tuneParams[['svmPoly']]),
    'nnet'= list(form=y ~ .,
                data = dataTrain,
                preProcess = c('scale','center'),
                method = 'nnet',
                trControl = fitControl,
                tuneGrid=tuneParams[['nnet']],
                linout=T),
    'rf'= list(form=y ~ .,
              data = dataTrain,
              preProcess = c('scale','center'),
              method = 'rf',
              trControl = fitControl,
              tuneGrid=tuneParams[['rf']],
              ntree=1e3),
    'knn'= list(form=y ~ .,
               data = dataTrain,
               preProcess = c('scale','center'),
               method = 'knn',
               trControl = fitControl,
               tuneGrid=tuneParams[['knn']]),
    'nb'= list(form=y ~ .,
              data = dataTrain,
              preProcess = c('scale','center'),
              method = 'nb',
              trControl = fitControl,
              tuneGrid=tuneParams[['nb']])
    # 'glm'= list(form=y ~ .,
    #            data = dataTrain,
    #            preProcess = c('scale','center'),
    #            method = 'glm',
    #            trControl = fitControl,
    #            tuneGrid=tuneParams[['glm']])
    # 'gam'= list(form=y ~ .,
    #            data = dataTrain,
    #            preProcess = c('scale','center'),
    #            method = 'gam',
    #            trControl = fitControl,
    #            tuneGrid=tuneParams[['gam']])
  )
    
  if (input$rdo_model_type == 'Regression') {
    reg_results <- lapply(mdls,function(m){
    do.call('train',trainArgs[[m]])
  })
  
  names(reg_results) <- mdls
  tuned_model0 <- reg_results
  tuned_model <<- tuned_model0
  } else {
    class_results <- list()
  
    class_models <- function(model, tuneParam){
      res <- caret::train(y ~., 
                          dataTrain, 
                          method = model, 
                          preProcess = c('scale','center'), 
                          trControl=fitControl, 
                          tuneGrid = tuneParam)
      return(res)
  }
  
    for (i in 1:sum(mdli$Classification)){
      class_results[[i]] <- class_models(model = cls.mdls[[i]], tuneParam = tuneParams[[i]])
    }
  
    names(class_results) <- cls.mdls
    tuned_model0 <- class_results
    tuned_model <- tuned_model0
  }
    tuned_model
})


train_test_predictions <- reactive({
  
  tuned_model <- train_models()
  
  # if(is.null(tuned_model)) return(NULL)
  if (input$rdo_model_type == 'Regression'){
        #Regression
    library(magrittr)

    fit_names <- c("RMSE", "Rsquared", "MAE","RMSESD","RsquaredSD","MAESD")

    list_df <- (purrr::map(tuned_model, 'results'))

    train_fit_summary <- list_df %>%
      purrr::map2_df(names(tuned_model),~dplyr::mutate(.x,name=.y)) %>%
      dplyr::select(name, dplyr::everything()) %>%
      tidyr::unite_('Model', names(.)[!names(.) %in% fit_names], sep='-', remove=T) %>%
      dplyr::mutate(Model = stringr::str_replace_all(Model, "-NA|-NA-|NA-|NA", "")) %>%
      dplyr::mutate(Rank = dplyr::dense_rank(Rsquared)) %>%
      dplyr::arrange(Rank) %>%
      dplyr::select(Rank, dplyr::everything())

    best_pred_df <- (purrr::map(tuned_model, 'bestTune')) %>%
      purrr::map2_df(names(tuned_model),~dplyr::mutate(.x,name=.y)) %>%
      dplyr::select(name, dplyr::everything()) %>%
      tidyr::unite_('Model', names(.), sep='-', remove=T) %>%
      dplyr::mutate(Model = stringr::str_replace_all(Model, "-NA|-NA-|NA-|NA", ""))

    pred_names <- c('pred','obs', 'rowIndex','Resample')
    list_pred_df <- (purrr::map(tuned_model, 'pred'))

    train_pred_summary <- list_pred_df %>%
      purrr::map2_df(names(tuned_model),~dplyr::mutate(.x,name=.y)) %>%
      dplyr::select(name, dplyr::everything()) %>%
      tidyr::unite_('Model', names(.)[!names(.) %in% pred_names], sep='-', remove=T) %>%
      dplyr::mutate(Model = stringr::str_replace_all(Model, "-NA|-NA-|NA-|NA", "")) %>%
      dplyr::inner_join(best_pred_df, by = "Model")

    list_cols <- (lapply(tuned_model[names(tuned_model)],predict.train,dataTest)) %>% data.frame()

    list_cols$y <- dataTest$y

    best_pred_df$Model_Name <- grep(pattern = "[a-z]+|[A-Z]+$", unlist(stringr::str_split(string = best_pred_df$Model, pattern = "-")),value = T)

    df_predicted <- tidyr::gather(do.call(cbind.data.frame, list_cols), Model_Name, Predicted, -y) %>%
      dplyr::inner_join(best_pred_df, by = c("Model_Name")) %>%
      dplyr::group_by(Model_Name) %>%
      dplyr::mutate(R_Square = round(sum((Predicted - mean(y))**2) / sum((y - mean(y))**2),4),
                       RMSE = round(sqrt(mean((y-Predicted)^2)),4)) %>%
      dplyr::mutate(Predicted = round(Predicted,4)) %>% 
      dplyr::ungroup()
  } else if (input$rdo_model_type == 'Classification') {
      # if(is.null(tuned_model)) return(NULL)

  library(magrittr)

  fit_names <- c("Accuracy", "Kappa", "AccuracySD", "KappaSD")

  list_df_class <- (purrr::map(tuned_model, 'results'))

  train_fit_summary <- list_df_class %>%
    purrr::map2_df(names(tuned_model),~dplyr::mutate(.x,name=.y)) %>%
    dplyr::select(name, dplyr::everything()) %>%
    tidyr::unite_('Model', names(.)[!names(.) %in% fit_names], sep='-', remove=T) %>%
    dplyr::mutate(Model = stringr::str_replace_all(Model, "-NA|-NA-|NA-|NA", "")) %>%
    dplyr::mutate(Rank = dplyr::dense_rank(Accuracy)) %>%
    dplyr::arrange(Rank) %>%
    dplyr::select(Rank, dplyr::everything())

  best_pred_df <- (purrr::map(tuned_model, 'bestTune')) %>%
    purrr::map2_df(names(tuned_model),~dplyr::mutate(.x,name=.y)) %>%
    dplyr::select(name, dplyr::everything()) %>%
    tidyr::unite_('Model', names(.), sep='-', remove=T) %>%
    dplyr::mutate(Model = stringr::str_replace_all(Model, "-NA|-NA-|NA-|NA", ""))


  pred_names <- c('pred','obs', 'rowIndex','Resample')
  list_pred_df <- (purrr::map(tuned_model, 'pred'))

  train_pred_summary <- list_pred_df %>%
    purrr::map2_df(names(tuned_model),~dplyr::mutate(.x,name=.y)) %>%
    dplyr::select(name, dplyr::everything()) %>%
    tidyr::unite_('Model', names(.)[!names(.) %in% pred_names], sep='-', remove=T) %>%
    dplyr::mutate(Model = stringr::str_replace_all(Model, "-NA|-NA-|NA-|NA", "")) %>%
    dplyr::inner_join(best_pred_df, by = "Model")

  list_cols <- (lapply(tuned_model[names(tuned_model)],predict.train,dataTest)) %>% data.frame()

  list_cols$y <- dataTest$y

  best_pred_df$Model_Name <- grep(pattern = "[a-z]+|[A-Z]+]$", unlist(stringr::str_split(string = best_pred_df$Model, pattern = "-|none|TRUE")),value = T)

  df_predicted <- tidyr::gather(do.call(cbind.data.frame, list_cols), Model_Name, Predicted, -y) %>%
    dplyr::inner_join(best_pred_df, by = c("Model_Name")) %>%
    dplyr::mutate(Predicted = as.factor(Predicted)) %>%
    dplyr::group_by(Model_Name) %>%
    dplyr::mutate(Accuracy = round(sum(Predicted == y)/length(y),4),
                  Kappa    = round(vcd::Kappa(table(Predicted,y))$Unweighted['value']),4) %>%
    dplyr::ungroup() %>% as.data.frame()
  }
  list(train_fit_summary=train_fit_summary, best_pred_df=best_pred_df,train_pred_summary=train_pred_summary,df_predicted=df_predicted )
})

```


#### Model Options
```{r}
sliderInput('sld_testsplit',label = 'Test Set %','lbl_testsplit',min = 33,max = 90,step = 1,value = 33)
radioButtons('rdo_model_type',label = 'Model Type',choices = c('Regression'= "Regression",'Classification'= "Classification"),inline = T)
selectInput('slt_algo',label = 'Algorithm',choices = reg.mdls,selected = reg.mdls,multiple=T)
selectizeInput('slt_Tune','Parameter Tuning',choices = c('Manual Grid Search')) #,'Coarse auto-tune (fast)','Fine auto-tune (slow)'))
radioButtons('rdo_CVtype',label = 'Cross-validation Folds',choices = c('3-fold'=3,'5-fold'=5,'10-fold'=10),inline = T)
                                         
actionButton('btn_train',label = 'Train Models',icon = icon('cogs'),class='btn-success fa-lg',width='100%')

```


#### Model Performance
```{r}
# final_models <- train_test_predictions()$best_pred_df
# selectInput('slt_Finalalgo',label = 'Final Model:'%>%label.help('lbl_Finalalgo'),choices=mdls,multiple=T),
# selectInput('slt_Finalalgo',label = 'Final Model',choices = final_models$Model, multiple=T)
renderUI({
  df <- train_test_predictions()$best_pred_df
  if (is.null(df)) return(NULL)

  items=(df$Model)
  selectInput(
    inputId = "slt_Finalalgo",
    label = "Final Model",
    choices = items,
    selected = items,
    multiple=T)
})

```

# Data Summary

## {.tabset}

### Outcome Distribution
```{r}
    renderPlot({
      ds <- rawdata() %>% as.data.frame()
      if ((class(ds[,input$yvar]) != 'factor')){
            TabularManifest::histogram_continuous(ds, 
                                          input$yvar, 
                                          main_title = paste0("Distribution of ",input$yvar),
                                          x_title = capitalize_each_word(input$yvar))
      } else {
           TabularManifest::histogram_discrete(ds, 
                                          input$yvar, 
                                          main_title = paste0("Distribution of ",input$yvar),
                                          x_title = capitalize_each_word(input$yvar))
      }

    })
```

### Plots of Predictor Variables
```{r}
    renderPlot({
      ds <- rawdata()
      
      if ((class(ds[,input$yvar]) == 'factor')) {
        GGally::ggpairs(ds, mapping = aes_string(color = input$yvar), columns = input$xvar)
        } else {
        GGally::ggpairs(ds, columns = c(input$yvar,input$xvar))
        }
    })

```

### Summary of Features
```{r}
    # renderPlot({
    #   ds <- rawdata()
    #   if (class(ds[input$yvar]) != "numeric") {
    #     GGally::ggpairs(ds, mapping = aes(color = input$yvar), columns = input$xvar)
    #     } else {
    #       GGally::ggpairs(ds)
    #     }
    # })


renderDataTable({
  ds <- rawdata()
  y <- input$yvar

   if (class(ds[,y]) == 'factor') {
    ds %>%
    tidyr::gather(key, value, -y) %>%  
    dplyr::group_by_(.dots = y, 'key') %>%
    dplyr::summarize(
       "Minimum"     = min(value, na.rm = TRUE),
       "Maximum"     = max(value, na.rm = TRUE),
       "Mean"        = mean(value, na.rm = TRUE),
       "No. of Obs." = sum(!is.na(value), na.rm = TRUE),
       "Std. Dev."   = round(sd(value, na.rm = TRUE),2),
       "Range"       = paste0("(", Minimum, " , ", Maximum, ")")
    ) %>%
    dplyr::rename("Features" = "key")

  } else {
    ds %>%
    # dplyr::select_(.dots = -y) %>%
    tidyr::gather(key, value) %>%
    dplyr::group_by(key) %>%
    dplyr::summarize(
       "Minimum"     = min(value, na.rm = TRUE),
       "Maximum"     = max(value, na.rm = TRUE),
       "Mean"        = mean(value, na.rm = TRUE),
       "No. of Obs." = sum(!is.na(value), na.rm = TRUE),
       "Std. Dev."   = round(sd(value, na.rm = TRUE),2),
       "Range"       = paste0("(", Minimum, " , ", Maximum, ")")
    ) %>%
    dplyr::rename("Features" = "key")
  }
 
})

```



# Training Models {.tabset data-width=300}

## {.tabset}

### CV Model Rank

```{r echo=echo_chunks, message=message_chunks}
renderPlot({
    resdf <- train_test_predictions()$train_fit_summary
    type <- isolate(input$rdo_model_type)
    
    if(type =='Regression'){
        ggplot(resdf,aes(x=Model,color=Model))+
          geom_errorbar(aes(ymin=RMSE-RMSESD,ymax=RMSE+RMSESD),size=1)+
          geom_point(aes(y=RMSE),size=3)+
          # scale_color_manual(values=pal)+
          coord_flip()+
          theme_bw()+
          xlab('')+
          theme(legend.position='none') -> p1
        ggplot(resdf,aes(x=Model,color=Model))+
          geom_errorbar(aes(ymin=Rsquared-RsquaredSD,ymax=Rsquared+RsquaredSD),size=1)+
          geom_point(aes(y=Rsquared),size=3)+
          # scale_color_manual(values=pal)+
          coord_flip()+
          theme_bw()+
          xlab('')+
          theme(legend.position='none') -> p2
      } else {
        ggplot(resdf,aes(x=Model, color=Model))+
          geom_errorbar(aes(ymin=Kappa-KappaSD,ymax=Kappa+KappaSD),size=1)+
          geom_point(aes(y=Kappa),size=3)+
          # scale_color_manual(values=pal)+
          coord_flip()+
          theme_bw()+
          xlab('')+
          theme(legend.position='none') -> p1
        ggplot(resdf,aes(x=Model,color=Model))+
          geom_errorbar(aes(ymin=Accuracy-AccuracySD,ymax=Accuracy+AccuracySD),size=1)+
          geom_point(aes(y=Accuracy),size=3)+
          # scale_color_manual(values=pal)+
          coord_flip()+
          theme_bw()+
          xlab('')+
          theme(legend.position='none') -> p2
      }
       gridExtra::grid.arrange(p2,p1,ncol=2)
  })

```

### CV Preds Vs. Obs

```{r echo=echo_chunks, message=message_chunks}
renderPlot({
    type <- isolate(input$rdo_model_type)
      df <- train_test_predictions()$train_pred_summary

    if(type=='Regression'){
      lims <- c(min(df$obs),max(df$obs))
      ggplot(df)+
        geom_abline(alpha=0.5)+
        geom_point(aes(x=obs,y=pred,col=Model))+
        scale_x_continuous(limits = lims)+
        scale_y_continuous(limits = lims)+
        # scale_color_manual(values=pal)+
        coord_equal()+
        facet_wrap(~Model)+
        theme_bw()+
        xlab('Observed')+
        ylab('Predicted')+
        theme(legend.position='none')
    } else {
      df %>% dplyr::group_by(pred,obs,Model) %>% 
        summarise(n=n()) %>% 
        ggplot(.)+
        geom_raster(aes(x=obs,y=pred,fill=Model,alpha=n))+
        geom_text(aes(x=obs,y=pred,label=n))+
        # scale_fill_manual(values=pal)+
        coord_equal()+
        facet_wrap(~Model)+
        theme_bw()+
        xlab('Observed')+
        ylab('Predicted')+
        theme(legend.position='none')
    }
})
```

### CV Statistics

```{r echo=echo_chunks, message=message_chunks}
DT::renderDataTable({
  train_test_predictions()$df_predicted
})
```


# Model Performance

## {.tabset}

### Test Set Predictions

```{r  echo=echo_chunks, message=message_chunks}
renderPlot({
    type <- isolate(input$rdo_model_type)
      df <- train_test_predictions()$df_predicted

    if(type=='Regression'){
      lims <- c(min(df$y),max(df$y))
      ggplot(df)+
        geom_abline(alpha=0.5)+
        geom_point(aes(x=y,y=Predicted,col=Model))+
        scale_x_continuous(limits = lims)+
        scale_y_continuous(limits = lims)+
        # scale_color_manual(values=pal)+
        coord_equal()+
        facet_wrap(~Model)+
        theme_bw()+
        xlab('Observed')+
        ylab('Predicted')+
        theme(legend.position='none')
    } else {
      df %>% dplyr::group_by(Predicted,y,Model) %>% 
        summarise(n=n()) %>% 
        ggplot(.)+
        geom_raster(aes(x=y,y=Predicted,fill=Model,alpha=n))+
        geom_text(aes(x=y,y=Predicted,label=n))+
        # scale_fill_manual(values=pal)+
        coord_equal()+
        facet_wrap(~Model)+
        theme_bw()+
        xlab('Observed')+
        ylab('Predicted')+
        theme(legend.position='none')
    }
})

```


### Test Set Evaluation Measures
```{r echo=echo_chunks, message=message_chunks}
DT::renderDataTable({
   type <- isolate(input$rdo_model_type)
   df <- train_test_predictions()$df_predicted
   df
})
```


### Feature Importance
```{r  echo=echo_chunks, message=message_chunks}
renderPlot({
    
    rf <- randomForest::randomForest(y~.,dataTrain)
    vi <- as.data.frame(randomForest::varImpPlot(rf))
    vi$Feature <- row.names(vi)
    names(vi)[1] <- 'Score'
    vi$Feature <- factor(vi$Feature,levels=vi$Feature[order(vi$Score)])
  str(vi)  
    ggplot(vi,aes(x=Feature,y=Score))+
      geom_bar(stat='identity',fill="#40D6A1")+
      coord_flip()+
      xlab('')+
      ylab('Relative Importance Score')
    
  })
```

# Documentation
## {.tabset}

### About the Dashboard

This dshaboard was build using `flexdashboard` package in R. This dashboard lets you to run a series of machine learning algorithms for both regression and classification problems. It also utilizes a manual grid search approach and a choice of three cross-validation folds to optimize the models.

#### Resources

[Link to Caret R Package](http://topepo.github.io/caret/index.html)
















