---
title: "Machine Learning Models"
params:
output:
  flexdashboard::flex_dashboard:
    social: [ "menu" ]
    orientation: rows
    vertical_layout: fill
    # source_code: embed
    theme: cerulean
runtime: shiny
---

<!--  Set the working directory to the repository's base directory; this assumes the report is nested inside of three directories.-->
```{r, echo=F, message=F}
# cat("Working directory: ", getwd())
library(knitr)
library(flexdashboard)
library(shiny)
library(caret)
opts_knit$set(root.dir='./../')  #Don't combine this call with any other chunk -especially one that uses file paths.
```

<!-- Set the report-wide options, and point to the external code file. -->
```{r set-options, echo=F}
# cat("Working directory: ", getwd())
report_render_start_time <- Sys.time()
opts_chunk$set(
  results      = 'show',
  comment      = NA,
  tidy         = FALSE,
  # dpi        = 400,
  # out.width  = "650px", #This affects only the markdown, not the underlying png file.  The height will be scaled appropriately.
  fig.width    = 4,
  fig.height   = 4,
  fig.path     = 'figure-dashboard-png/'
)

echo_chunks    <- FALSE #Toggle for debugging.
message_chunks <- FALSE #Toggle for debugging.
options(width=100) #So the output is 25% wider than the default.
```



```{r}
datasetss <- list(
  'none'= NULL,
  'iris'= datasets::iris,
  'cars'= datasets::mtcars,
  # 'diamonds'= ggplot2::diamonds,
  'Boston'=MASS::Boston
  # 'leaf'=leaf
  # 'midwest'=data.frame(midwest),
  # 'mpg'=data.frame(mpg),
  # 'msleep'=data.frame(msleep),
  # 'txhousing'=data.frame(txhousing)
)

mdls <- list('svmLinear'='svmLinear',
             'svmPoly'='svmPoly',
             'Neural Network'='nnet',
             'randomForest'='rf',
             'k-NN'='knn',
             'Naive Bayes'='nb',
             'GLM'='glm'
             )

tuneParams <- list(
  'svmLinear'=data.frame(C=c(0.01,0.1,1)),
  'svmPoly'= expand.grid(degree=1:3,scale=c(0.01,0.1),C=c(0.25,0.5,1)),
  'nnet'=expand.grid(size=c(1,3,5),decay=c(0.01,0.1,1)),
  'rf'=data.frame(mtry=c(2,3,4)),
  'knn'=data.frame(k=c(1,3,5,7,9)),
  'nb'=expand.grid(usekernel=c(T,F),adjust=c(0.01,0.1,1),fL=c(0.01,0.1,1)),
  'glm'=NULL#data.frame()
)

mdli <- list(
  'Regression'=c(T,T,T,T,T,F,F),
  'Classification'=c(T,T,T,T,T,T,T)
)  

reg.mdls <- mdls[mdli[['Regression']]]
cls.mdls <- mdls[mdli[['Classification']]]

capitalize_each_word <- function(string, pattern = NULL){
  if (!is.null(pattern)) {
    g <- l <- j <- list()
    g <- stringr::str_split(base::trimws(string), pattern = pattern)
    for ( i in 1:length(g)){
      l[[i]] <- paste(Hmisc::capitalize(g[[i]]), collapse = " ")
      j[[i]] <- paste(l[[i]], collapse = " ")
    }
    } else {
    g <- l <- j <- list()
    g <- stringr::str_split(base::trimws(string), pattern = "\\s+")
    for ( i in 1:length(g)){
      l[[i]] <- paste(Hmisc::capitalize(g[[i]]), collapse = " ")
      j[[i]] <- paste(l[[i]], collapse = " ")
  }
}
  return(unlist(j))
}
```


<!-- Load 'sourced' R files.  Suppress the output when loading sources. -->
```{r load-sources, echo=echo_chunks, message=message_chunks}
```

<!-- Load packages, or at least verify they're available on the local machine.  Suppress the output when loading packages. -->
```{r load-packages, echo=echo_chunks, message=message_chunks}
```

<!-- Load any global functions and variables declared in the R file.  Suppress the output. -->
```{r declare-globals, echo=echo_chunks, results='show', message=message_chunks}
```

<!-- Declare any global functions specific to a Rmd output.  Suppress the output. -->
```{r rmd-specific, echo=echo_chunks, message=message_chunks}
```

<!-- Load the datasets.   -->
```{r load-data, echo=echo_chunks, results='show', message=message_chunks}
```

<!-- Tweak the datasets.   -->
```{r tweak-data, echo=echo_chunks, results='show', message=message_chunks}
```


Sidebar {.sidebar data-width=300}
=======================================================================

```{r}
library(shiny)
library(shinydashboard)
library(shinyjs)
require(shinyBS)
library("htmltools")
library("bsplus")
library(magrittr)
```

#### Data Input
```{r}
selectInput('dataset',label = 'Choose Dataset',choices = names(datasetss),selected='none')
fileInput('fileIn',label = 'Upload Data')
actionButton('btn_viewData',label = 'View Data',icon=icon('table'), width = '100%')
shiny::tags$br()
shiny::tags$br()
# selectizeInput('yvar',label='y (Choose Outcome Variable)',choices = character(0))
# selectizeInput('xvar',label='X (Select Predictor Variables)',choices = character(0),multiple = T)

```

```{r}
rawdata <- reactive({
  if (input$dataset == 'none'){
      input_file <- input$fileIn
      if(is.null(input_file)){return()}
      read.csv(
        file = input_file$datapath,
        sep = ','
      )
  } else {
    datasetss[[input$dataset]]
  }
})

bsModal('data',title = 'Dataset',trigger = 'btn_viewData',size = 'large',renderDataTable({rawdata()}))

renderUI({
  df <- rawdata()
  if (is.null(df)) return(NULL)

  items=names(df)
  names(items)=items
  selectInput(
    inputId = "yvar",
    label = "y (Choose Outcome Variable)",
    choices = items)
})

renderUI({
    df <- rawdata()
    if (is.null(df)) return(NULL)

    items=names(df)
    names(items)=items
    selectInput(
      inputId = "xvar",
      label = "X (Select Predictor Variables)",
      multiple = T,
      choices  = names(rawdata())[!names(rawdata()) %in% input$yvar],
      selected = names(rawdata())[!names(rawdata()) %in% input$yvar][1])
  })

reactive({
  observeEvent(input$dataset, {
   updateSelectizeInput(session,'yvar',choices=names(rawdata()),selected = names(rawdata())[1])
})
})

reactive({
  observeEvent(input$fileIn, {
   updateSelectizeInput(session,'yvar',choices=names(rawdata()),selected = names(rawdata())[1])
})
})

reactive({
observeEvent(input$yvar, {
  nms <- names(rawdata())[names(rawdata())!=input$yvar]
  updateSelectizeInput(session,'xvar',choices=nms,selected = nms)
})
})


dataTrain <- NULL
dataTest <- NULL

makeReactiveBinding('dataTrain')
makeReactiveBinding('dataTest')

observeEvent(input$rdo_model_type,{
  
  if(input$rdo_model_type=='Regression'){
    updateSelectizeInput(session,'slt_algo',choices = reg.mdls,selected = reg.mdls)
  } else {
    updateSelectizeInput(session,'slt_algo',choices = cls.mdls,selected = cls.mdls)
    
  }
})

observe({
    
    yvar <- input$yvar
    xvars <- input$xvar
    testsize <- input$sld_testsplit
    
    if(is.null(yvar)||yvar=='')
      return(NULL)
    
    # extract y and X from raw data
    y <- isolate(rawdata()[,yvar])
    X <-  isolate(rawdata()[,xvars])
    
    # deal with NA values
    yi <- !is.na(y)
    Xi <- complete.cases(X)
    
    df2 <- as.data.frame(cbind(y,X)[yi&Xi,])

    trainIndex <- caret::createDataPartition(df2$y,
                                      p = 1-(testsize/100),
                                      list = FALSE,
                                      times = 1)
    isolate({
      dataTrain <<- df2[ trainIndex,]
      dataTest  <<- df2[-trainIndex,]
    })
  })


observeEvent(input$btn_train,{
    
    disable('btn_train')
    on.exit(enable('btn_train'))
    
    mdls <- isolate(input$slt_algo)

    fitControl <- caret::trainControl(method = "cv",savePredictions = T,
                           number = as.integer(input$rdo_CVtype),
                           summaryFunction = defaultSummary)
      
    trainArgs <- list(
    'svmLinear'= list(form=y ~ .,
                     data = dataTrain,
                     preProcess = c('scale','center'),
                     method = 'svmLinear',
                     trControl = fitControl,
                     tuneGrid=tuneParams[['svmLinear']]),
    'svmPoly'= list(form=y ~ .,
                    data = dataTrain,
                    preProcess = c('scale','center'),
                    method = 'svmPoly',
                    trControl = fitControl,
                    tuneGrid=tuneParams[['svmPoly']]),
    'nnet'= list(form=y ~ .,
                data = dataTrain,
                preProcess = c('scale','center'),
                method = 'nnet',
                trControl = fitControl,
                tuneGrid=tuneParams[['nnet']],
                linout=T),
    'rf'= list(form=y ~ .,
              data = dataTrain,
              preProcess = c('scale','center'),
              method = 'rf',
              trControl = fitControl,
              tuneGrid=tuneParams[['rf']],
              ntree=1e3),
    'knn'= list(form=y ~ .,
               data = dataTrain,
               preProcess = c('scale','center'),
               method = 'knn',
               trControl = fitControl,
               tuneGrid=tuneParams[['knn']]),
    'nb'= list(form=y ~ .,
              data = dataTrain,
              preProcess = c('scale','center'),
              method = 'nb',
              trControl = fitControl,
              tuneGrid=tuneParams[['nb']]),
    'glm'= list(form=y ~ .,
               data = dataTrain,
               preProcess = c('scale','center'),
               method = 'glm',
               trControl = fitControl,
               tuneGrid=tuneParams[['glm']])
    # 'gam'= list(form=y ~ .,
    #            data = dataTrain,
    #            preProcess = c('scale','center'),
    #            method = 'gam',
    #            trControl = fitControl,
    #            tuneGrid=tuneParams[['gam']])
  )
    
  if (input$rdo_model_type == 'Regression') {
    reg_results <- lapply(mdls,function(m){
    do.call('train',trainArgs[[m]])
  })
  
  names(reg_results) <- mdls
  tuned_model0 <- reg_results
  tuned_model <<- tuned_model0
  } else {
    class_results <- list()
  
    class_models <- function(model, tuneParam){
      res <- caret::train(y ~., 
                          dataTrain, 
                          method = model, 
                          preProcess = c('scale','center'), 
                          trControl=fitControl, 
                          tuneGrid = tuneParam)
      return(res)
  }
  
    for (i in 1:sum(mdli$Classification)){
      class_results[[i]] <- class_models(model = cls.mdls[[i]], tuneParam = tuneParams[[i]])
    }
  
    names(class_results) <- cls.mdls
    tuned_model0 <- class_results
    tuned_model <<- tuned_model0
  }
})

dff <- reactive({
  df <- tuned_model()
  df
})


# train_test_predictions <- reactive({
#   if(is.null(tuned_model)) return(NULL)
#   if (input$rdo_model_type == 'Regression'){
#         #Regression
# library(magrittr)
# 
#     fit_names <- c("RMSE", "Rsquared", "MAE","RMSESD","RsquaredSD","MAESD")
# 
#     list_df <- (purrr::map(tuned_model, 'results'))
# 
#     train_fit_summary <- list_df %>% 
#       purrr::map2_df(names(tuned_model),~dplyr::mutate(.x,name=.y)) %>% 
#       dplyr::select(name, dplyr::everything()) %>% 
#       tidyr::unite_('Model', names(.)[!names(.) %in% fit_names], sep='-', remove=T) %>% 
#       dplyr::mutate(Model = stringr::str_replace_all(Model, "-NA|-NA-|NA-|NA", "")) %>% 
#       dplyr::mutate(rank = dplyr::dense_rank(Rsquared)) %>% 
#       dplyr::arrange(rank) %>% 
#       dplyr::select(rank, dplyr::everything())
#     
#     best_pred_df <- (purrr::map(tuned_model, 'bestTune')) %>% 
#       purrr::map2_df(names(tuned_model),~dplyr::mutate(.x,name=.y)) %>% 
#       dplyr::select(name, dplyr::everything()) %>% 
#       tidyr::unite_('Model', names(.), sep='-', remove=T) %>% 
#       dplyr::mutate(Model = stringr::str_replace_all(Model, "-NA|-NA-|NA-|NA", "")) 
#     
#     pred_names <- c('pred','obs', 'rowIndex','Resample')
#     list_pred_df <- (purrr::map(tuned_model, 'pred'))
#     
#     train_pred_summary <- list_pred_df %>%
#       purrr::map2_df(names(tuned_model),~dplyr::mutate(.x,name=.y)) %>%
#       dplyr::select(name, dplyr::everything()) %>%
#       tidyr::unite_('Model', names(.)[!names(.) %in% pred_names], sep='-', remove=T) %>%
#       dplyr::mutate(Model = stringr::str_replace_all(Model, "-NA|-NA-|NA-|NA", "")) %>%
#       dplyr::inner_join(best_pred_df, by = "Model")
#     
#     list_cols <- (lapply(tuned_model[names(tuned_model)],predict.train,dataTest)) %>% data.frame()
#     
#     list_cols$y <- dataTest$y
#     
#     best_pred_df$model_name <- grep(pattern = "[a-z]+|[A-Z]+$", unlist(stringr::str_split(string = best_pred_df$Model, pattern = "-")),value = T)
#     
#     df_predicted <- tidyr::gather(do.call(cbind.data.frame, list_cols), model_name, predicted, -y) %>% 
#       dplyr::inner_join(best_pred_df, by = c("model_name")) %>% 
#       dplyr::group_by(model_name) %>% 
#       dplyr::mutate(r_square = sum((predicted - mean(y))**2) / sum((y - mean(y))**2),
#                        rmse = sqrt(mean((y-predicted)^2))) %>% 
#       dplyr::ungroup()
#   } else {
#       if(is.null(tuned_model)) return(NULL)
# 
#   library(magrittr)
# 
#   fit_names <- c("Accuracy", "Kappa", "AccuracySD", "KappaSD")
#   
#   list_df_class <- (purrr::map(tuned_model, 'results'))
#   
#   train_fit_summary <- list_df_class %>% 
#     purrr::map2_df(names(tuned_model),~dplyr::mutate(.x,name=.y)) %>% 
#     dplyr::select(name, dplyr::everything()) %>% 
#     tidyr::unite_('Model', names(.)[!names(.) %in% fit_names], sep='-', remove=T) %>% 
#     dplyr::mutate(Model = stringr::str_replace_all(Model, "-NA|-NA-|NA-|NA", "")) %>% 
#     dplyr::mutate(rank = dplyr::dense_rank(Accuracy)) %>% 
#     dplyr::arrange(rank) %>% 
#     dplyr::select(rank, dplyr::everything())
#   
#   best_pred_df <- (purrr::map(tuned_model, 'bestTune')) %>% 
#     purrr::map2_df(names(tuned_model),~dplyr::mutate(.x,name=.y)) %>% 
#     dplyr::select(name, dplyr::everything()) %>% 
#     tidyr::unite_('Model', names(.), sep='-', remove=T) %>% 
#     dplyr::mutate(Model = stringr::str_replace_all(Model, "-NA|-NA-|NA-|NA", "")) 
#   
#   
#   pred_names <- c('pred','obs', 'rowIndex','Resample')
#   list_pred_df <- (purrr::map(tuned_model, 'pred'))
#   
#   train_pred_summary <- list_pred_df %>%
#     purrr::map2_df(names(tuned_model),~dplyr::mutate(.x,name=.y)) %>%
#     dplyr::select(name, dplyr::everything()) %>%
#     tidyr::unite_('Model', names(.)[!names(.) %in% pred_names], sep='-', remove=T) %>%
#     dplyr::mutate(Model = stringr::str_replace_all(Model, "-NA|-NA-|NA-|NA", "")) %>%
#     dplyr::inner_join(best_pred_df, by = "Model")
#   
#   list_cols <- (lapply(tuned_model[names(tuned_model)],predict.train,dataTest)) %>% data.frame()
#   
#   list_cols$y <- dataTest$y
#   
#   best_pred_df$model_name <- grep(pattern = "[a-z]+|[A-Z]+]$", unlist(stringr::str_split(string = best_pred_df$Model, pattern = "-|none|TRUE")),value = T)
#   
#   df_predicted <- tidyr::gather(do.call(cbind.data.frame, list_cols), model_name, predicted, -y) %>% 
#     dplyr::inner_join(best_pred_df, by = c("model_name")) %>% 
#     dplyr::mutate(predicted = as.factor(predicted)) %>% 
#     dplyr::group_by(model_name) %>% 
#     dplyr::mutate(Accuracy = sum(predicted == y)/length(y),
#                   Kappa    = vcd::Kappa(table(predicted,y))$Unweighted['value']) %>% 
#     dplyr::ungroup() %>% as.data.frame()
#   }
#   list(train_fit_summary=train_fit_summary, best_pred_df=best_pred_df,train_pred_summary=train_pred_summary,df_predicted=df_predicted )
# })

```


#### Model Options
```{r}
sliderInput('sld_testsplit',label = 'Test Set %','lbl_testsplit',min = 33,max = 90,step = 1,value = 33)
radioButtons('rdo_model_type',label = 'Model Type',choices = c('Regression'= "Regression",'Classification'= "Classification"),inline = T)
selectInput('slt_algo',label = 'Algorithm',choices = reg.mdls,selected = reg.mdls,multiple=T)
selectizeInput('slt_Tune','Parameter Tuning',choices = c('Manual Grid Search')) #,'Coarse auto-tune (fast)','Fine auto-tune (slow)'))
radioButtons('rdo_CVtype',label = 'Cross-validation Folds',choices = c('3-fold'=3,'5-fold'=5,'10-fold'=10),inline = T)
                                         
actionButton('btn_train',label = 'Train Models',icon = icon('cogs'),class='btn-success fa-lg',width='100%')

```


#### Model Performance
```{r}
# selectInput('slt_Finalalgo',label = 'Final Model:'%>%label.help('lbl_Finalalgo'),choices=mdls,multiple=T),
selectInput('slt_Finalalgo',label = 'Final Model',choices = mdls, multiple=F)
```

# Data Summary

## {.tabset}

### Outcome Distribution
```{r}
    renderPlot({
      ds <- rawdata() %>% as.data.frame()
      if ((class(ds[,input$yvar]) != 'factor')){
            TabularManifest::histogram_continuous(ds, 
                                          input$yvar, 
                                          main_title = paste0("Distribution of ",input$yvar),
                                          x_title = capitalize_each_word(input$yvar))
      } else {
           TabularManifest::histogram_discrete(ds, 
                                          input$yvar, 
                                          main_title = paste0("Distribution of ",input$yvar),
                                          x_title = capitalize_each_word(input$yvar))
      }

    })
```

### Plots of Predictor Variables
```{r}
    renderPlot({
      ds <- rawdata()
      
      if ((class(ds[,input$yvar]) == 'factor')) {
        GGally::ggpairs(ds, mapping = aes_string(color = input$yvar), columns = input$xvar)
        } else {
        GGally::ggpairs(ds, columns = c(input$yvar,input$xvar))
        }
    })

```

### Summary of Features
```{r}
    # renderPlot({
    #   ds <- rawdata()
    #   if (class(ds[input$yvar]) != "numeric") {
    #     GGally::ggpairs(ds, mapping = aes(color = input$yvar), columns = input$xvar)
    #     } else {
    #       GGally::ggpairs(ds)
    #     }
    # })


renderDataTable({
  ds <- rawdata()
  y <- input$yvar

   if (class(ds[,y]) == 'factor') {
    ds %>%
    tidyr::gather(key, value, -y) %>%  
    dplyr::group_by_(.dots = y, 'key') %>%
    dplyr::summarize(
       "Minimum"     = min(value, na.rm = TRUE),
       "Maximum"     = max(value, na.rm = TRUE),
       "Mean"        = mean(value, na.rm = TRUE),
       "No. of Obs." = sum(!is.na(value), na.rm = TRUE),
       "Std. Dev."   = round(sd(value, na.rm = TRUE),2),
       "Range"       = paste0("(", Minimum, " , ", Maximum, ")")
    ) %>%
    dplyr::rename("Features" = "key")

  } else {
    ds %>%
    # dplyr::select_(.dots = -y) %>%
    tidyr::gather(key, value) %>%
    dplyr::group_by(key) %>%
    dplyr::summarize(
       "Minimum"     = min(value, na.rm = TRUE),
       "Maximum"     = max(value, na.rm = TRUE),
       "Mean"        = mean(value, na.rm = TRUE),
       "No. of Obs." = sum(!is.na(value), na.rm = TRUE),
       "Std. Dev."   = round(sd(value, na.rm = TRUE),2),
       "Range"       = paste0("(", Minimum, " , ", Maximum, ")")
    ) %>%
    dplyr::rename("Features" = "key")
  }
 
})

```



# Training Models {.tabset data-width=300}

## {.tabset}

### CV Model Rank

```{r echo=echo_chunks, message=message_chunks}
DT::renderDataTable({
dff()
})
```

### CV Preds Vs. Obs

```{r echo=echo_chunks, message=message_chunks}

```

### CV Statistics

```{r echo=echo_chunks, message=message_chunks}

```


# Model Performance

## {.tabset}

### Test Set Predictions
```{r  echo=echo_chunks, message=message_chunks}

```

### Feature Importance
```{r  echo=echo_chunks, message=message_chunks}

```

# Documentation

## {.tabset}

### Explanation - Current Model

1. Measures

    * **Outcome**: Tracking goal progress reported on the Encounter Form and Goal Plan Form in ETO
        * **Numerator**: Numberof PDSA3/HOPE goals where progress (“started”, “partially completed”, or “completed”) has occurred.
        * **Denominator**: Total number of actively pursued, PDSA3/HOPE goals

    * **Process**: Submit goal status in the Goal Plan Form: Abandoned, Not Started, Started, Partially Completed, and Completed. 
        * **Numerator**: Weekly discussions towards PDSA3/HOPE goals
        * **Denominator**: Total number of actively pursued, PDSA3/HOPE goals
    
    * **Disruptor**: Not able to establish the goal (families without an active goal)
        * **Numerator**: Number of clients that have not established goals
        * **Denominator**: Total number of goals (sum of discussed and not discussed)

2. A program's weekly values are plotted on top of the background of other programs in the model.

3. Each graph represents one outcome. 

#### Resources

[Link to Current PDSA Document in Google Drive](https://docs.google.com/document/d/1A8sSklKjF3-1stEkRMFxpRYN2K7TrsFY7q-j9OusXB4/edit)

### Explanation -All Dashboards

















